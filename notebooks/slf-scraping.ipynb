{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "# Data scraping\n",
    "\n",
    "This notebook was used to scrap all the files of interest on the [SLF archive website](https://www.slf.ch/fr/bulletin-davalanches-et-situation-nivologique/archives.html?tx_wslavalanches_archiv%5Bpath%5D=%2Fuser_upload%2Fimport%2Flwdarchiv%2Fpublic%2F&tx_wslavalanches_archiv%5Baction%5D=showArchiv&tx_wslavalanches_archiv%5Bcontroller%5D=Avalanche&cHash=c71751a643ec4629e21b0306033ccd59). We first define some utility functions to extract all the directory hierarchy from the HTML pages. Then we filter which files we are interested into and dump their URL into a [file](./files_to_download). We finally download each file using a small multithreaded [python script](../src/download.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from typing import Dict, List\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_url = 'https://www.slf.ch/'\n",
    "archive_parent = 'fr/bulletin-davalanches-et-situation-nivologique/archives.html?tx_wslavalanches_archiv%5Bpath%5D=%2Fuser_upload%2Fimport%2Flwdarchiv%2Fpublic%2F&tx_wslavalanches_archiv%5Baction%5D=showArchiv&tx_wslavalanches_archiv%5Bcontroller%5D=Avalanche&cHash=c71751a643ec4629e21b0306033ccd59'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_folders(url: str) -> Dict[str, str]:\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    folders = [folder.find('a') for folder in soup.findAll(class_='folder')]\n",
    "    return {str(folder.contents[2]).split(',')[0].strip(): folder['href'] for folder in folders}\n",
    "\n",
    "def extract_folders_rec(url: str, max_level: int = 3, curr_level: int = 0):\n",
    "    full_url = base_url + url\n",
    "    subfolders = extract_folders(full_url)\n",
    "    if curr_level < max_level:\n",
    "        for subfolder in subfolders:\n",
    "            subfolder_url = subfolders[subfolder]\n",
    "            subfolders[subfolder] = extract_folders_rec(subfolder_url, max_level, curr_level + 1)\n",
    "    return subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6251e4e7dbed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhierarchy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_folders_rec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0marchive_parent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhierarchy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubfolder_url\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhierarchy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubfolder_url\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhierarchy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'folder'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'subfolder'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mhierarchy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhierarchy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'folder'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'subfolder'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mhierarchy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# show first level of archive\n",
    "hierarchy = extract_folders_rec(base_url + archive_parent, max_level=1)\n",
    "hierarchy = [(folder, subfolder, subfolder_url) for folder, content in hierarchy.items() for subfolder, subfolder_url in content.items()]\n",
    "hierarchy = pd.DataFrame(data, columns=['folder', 'subfolder', 'url'])\n",
    "hierarchy = hierarchy.set_index(['folder', 'subfolder'])\n",
    "hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are filters for which folders to extract. It follows the following rules:\n",
    "\n",
    "- **language:** files are often duplicated for the 4 languages (de, fr, it, en). When it is the case we download only one set in the following order of preference: en - fr - de. German is the default (always present).\n",
    "- **too specific:** some files are not interesting for now (too specific or too regional). We don't download the snowprofiles and the regional snow report,\n",
    "- **color or black and white:** maps are available in color and in black and white. Colors are easier for computer vision algorithm, so we drop the black and white map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def folders_filter(folders: Dict[str, str]) -> Dict[str, str]:\n",
    "    # language picking\n",
    "    if 'en' in folders:\n",
    "        return {'en': folders['en']}\n",
    "    if 'fr' in folders:\n",
    "        return {'fr': folders['fr']}\n",
    "    if 'de' in folders:\n",
    "        return {'de': folders['de']}\n",
    "    \n",
    "    new_folders = folders.copy()\n",
    "    for key in folders:\n",
    "        if 'regional' in key.lower() or 'régional' in key.lower():\n",
    "            new_folders.pop(key)\n",
    "        if 'icône' in key.lower() or 'icone' in key.lower():\n",
    "            new_folders.pop(key)\n",
    "        if 'Schneedeckenstabilität' in key:\n",
    "            new_folders.pop(key)\n",
    "    return new_folders\n",
    "\n",
    "def files_filter(files_url: List[str]) -> List[str]:\n",
    "    new_files_url = []\n",
    "    for fu in files_url:\n",
    "        f = path.basename(fu)\n",
    "        if 'bw' not in f or 'bw.txt' in f:\n",
    "            new_files_url.append(fu)\n",
    "    return new_files_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "import os\n",
    "\n",
    "def extract_files(url: str):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    files = [folder.find('a')['href'] for folder in soup.findAll(class_='linkedListPoint')]\n",
    "    return files\n",
    "\n",
    "# u = 'https://www.slf.ch/fr/bulletin-davalanches-et-situation-nivologique/archives.html?tx_wslavalanches_archiv%5Bpath%5D=%2Fuser_upload%2Fimport%2Flwdarchiv%2Fpublic%2F2018%2Fhstop%2Ffr%2Fgif%2F&tx_wslavalanches_archiv%5Baction%5D=showArchiv&tx_wslavalanches_archiv%5Bcontroller%5D=Avalanche&cHash=3a2d286c7428ec5abc465a7412ad6f65'\n",
    "# extract_files(u)\n",
    "\n",
    "def fetch_all(url: str, dest: str, curr_path: str = '', count: int = 0):\n",
    "    fs = extract_files(url)\n",
    "    fs = files_filter(fs)\n",
    "    for file_url in fs:\n",
    "        dest_file = path.join(dest, curr_path, path.basename(file_url))\n",
    "        if not os.path.exists(dest_file):\n",
    "            content = requests.get(path.join(base_url, file_url)).content\n",
    "            with open(dest_file, 'wb') as f:\n",
    "                f.write(content)\n",
    "            count += 1\n",
    "        print('count {}\\t{}'.format(count, path.join(curr_path, path.basename(file_url))), end='\\r')\n",
    "\n",
    "    sub_directories = extract_folders(url)\n",
    "    sub_directories = folders_filter(sub_directories)\n",
    "    for name, sub_url in sub_directories.items():\n",
    "        new_path = path.join(curr_path, name)\n",
    "        create_dir(path.join(dest, new_path))\n",
    "        count = fetch_all(base_url + sub_url, dest, curr_path=new_path, count=count)\n",
    "        \n",
    "    return count\n",
    "\n",
    "def create_dir(d):\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def files_generator(url: str):\n",
    "    fs = extract_files(url)\n",
    "    fs = files_filter(fs)\n",
    "    for file_url in fs:\n",
    "        yield file_url\n",
    "\n",
    "    sub_directories = extract_folders(url)\n",
    "    sub_directories = folders_filter(sub_directories)\n",
    "    for name, sub_url in sub_directories.items():\n",
    "        for f in files_generator(base_url + sub_url):\n",
    "            yield f\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entry_point = 'https://www.slf.ch/fr/bulletin-davalanches-et-situation-nivologique/archives.html?tx_wslavalanches_archiv%5Bpath%5D=%2Fuser_upload%2Fimport%2Flwdarchiv%2Fpublic%2F&tx_wslavalanches_archiv%5Baction%5D=showArchiv&tx_wslavalanches_archiv%5Bcontroller%5D=Avalanche&cHash=c71751a643ec4629e21b0306033ccd59'\n",
    "destination = '../data2/'\n",
    "# no bw, en-fr-de in order, no profile, no regional, no icone\n",
    "\n",
    "# fetch_all(entry_point, destination)\n",
    "with open('files_to_download', 'w') as dest:\n",
    "    dest.writelines(map(lambda x: base_url + x + '\\n', files_generator(entry_point)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the python script `../src/dowload.py` to fetch the ~30'000 files in the directory structure.\n",
    "\n",
    "```\n",
    "python3 src/download.py notebooks/files_to_download ./data/slf --prefix https://www.slf.ch/fileadmin/user_upload/import/lwdarchiv/public/ --nproc 4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
